{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/hitesh.goel/miniconda3/envs/inlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import glob\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import nltk\n",
    "import collections\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import torch\n",
    "import argparse\n",
    "import time\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import networkx as nx\n",
    "import pickle as pkl\n",
    "import dgl \n",
    "from module.vocabulary import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../cnndm'\n",
    "cache_dir = '../cache/CNNDM'\n",
    "DATA_FILE = os.path.join(data_dir, \"index_to_file_mapping_train.json\")\n",
    "VALID_FILE = os.path.join(data_dir, \"index_to_file_mapping_val.json\")\n",
    "VOCAL_FILE = os.path.join(cache_dir, \"vocab\")\n",
    "FILTER_WORD = os.path.join(cache_dir, \"filter_word.txt\")\n",
    "train_w2s_path = os.path.join(cache_dir, \"index_to_file_mapping_train.json\")\n",
    "val_w2s_path = os.path.join(cache_dir, \"index_to_file_mapping_val.json\")\n",
    "    \n",
    "# defaults\n",
    "vocab_size = 50000\n",
    "doc_max_timesteps = 50\n",
    "sent_max_len = 100\n",
    "\n",
    "# vocab\n",
    "vocab = Vocab(VOCAL_FILE, vocab_size)\n",
    "\n",
    "# filterwords\n",
    "FILTERWORD = stopwords.words('english')\n",
    "punctuations = [',', '.', ':', ';', '?', '(', ')', '[', ']', '&', '!', '*', '@', '#', '$', '%', '\\'\\'', '\\'', '`', '``',\n",
    "                '-', '--', '|', '\\/']\n",
    "FILTERWORD.extend(punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def readJson(fname):\n",
    "    data = []\n",
    "    with open(fname, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "\n",
    "def readText(fname):\n",
    "    data = []\n",
    "    with open(fname, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data.append(line.strip())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Example(object):\n",
    "    def __init__(self, article_sents, abstract_sents, vocab, sent_max_len, label):\n",
    "        self.sent_max_len = sent_max_len\n",
    "        self.enc_sent_len = []\n",
    "        self.enc_sent_input = []\n",
    "        self.enc_sent_input_pad = []\n",
    "\n",
    "        # Store the original strings\n",
    "        self.original_article_sents = article_sents\n",
    "        self.original_abstract = \"\\n\".join(abstract_sents)\n",
    "\n",
    "        # Process the article\n",
    "        if isinstance(article_sents, list) and isinstance(article_sents[0], list):  # multi document\n",
    "            self.original_article_sents = []\n",
    "            for doc in article_sents:\n",
    "                self.original_article_sents.extend(doc)\n",
    "        for sent in self.original_article_sents:\n",
    "            article_words = sent.split()\n",
    "            self.enc_sent_len.append(len(article_words))  # store the length before padding\n",
    "            self.enc_sent_input.append([vocab.word2id(w.lower()) for w in article_words])  # list of word ids; OOVs are represented by the id for UNK token\n",
    "        self._pad_encoder_input(vocab.word2id('[PAD]'))\n",
    "\n",
    "        # Store the label\n",
    "        self.labels = np.array([1 if i in label else 0 for i in range(len(self.original_article_sents))])\n",
    "        \n",
    "    def _pad_encoder_input(self, pad_id):\n",
    "        \"\"\"\n",
    "        :param pad_id: int; token pad id\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        max_len = self.sent_max_len\n",
    "        for i in range(len(self.enc_sent_input)):\n",
    "            article_words = self.enc_sent_input[i].copy()\n",
    "            if len(article_words) > max_len:\n",
    "                article_words = article_words[:max_len]\n",
    "            if len(article_words) < max_len:\n",
    "                article_words.extend([pad_id] * (max_len - len(article_words)))\n",
    "            self.enc_sent_input_pad.append(article_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_path, vocab, doc_max_timesteps, sent_max_len, filter_word_path, w2s_path, num_topics):\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.sent_max_len = sent_max_len\n",
    "        self.doc_max_timesteps = doc_max_timesteps\n",
    "        self.num_topics = num_topics\n",
    "        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.example_list = json.load(f)\n",
    "        self.size = len(self.example_list)\n",
    "\n",
    "        tfidf_w = readText(filter_word_path)\n",
    "        self.filterwords = FILTERWORD\n",
    "        self.filterids = [vocab.word2id(w.lower()) for w in FILTERWORD]\n",
    "        self.filterids.append(vocab.word2id(\"[PAD]\"))   # keep \"[UNK]\" but remove \"[PAD]\"\n",
    "        lowtfidf_num = 0\n",
    "        for w in tfidf_w:\n",
    "            if vocab.word2id(w) != vocab.word2id('[UNK]'):\n",
    "                self.filterwords.append(w)\n",
    "                self.filterids.append(vocab.word2id(w))\n",
    "                lowtfidf_num += 1\n",
    "            if lowtfidf_num > 5000:\n",
    "                break\n",
    "        self.filterids = list(set(self.filterids))\n",
    "        self.filterwords = list(set(self.filterwords))\n",
    "        with open(w2s_path, \"r\") as f:\n",
    "            self.w2s_tfidf = json.load(f)  \n",
    "            \n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')      \n",
    "            \n",
    "    def get_example(self, index):\n",
    "        file_name, new_index  = self.example_list[str(index)]\n",
    "        e = readJson(file_name)\n",
    "        e = e[new_index]\n",
    "        e[\"summary\"] = e.setdefault(\"summary\", [])\n",
    "        self.text = e['text']\n",
    "        example = Example(e[\"text\"], e[\"summary\"], self.vocab, self.sent_max_len, e[\"label\"])\n",
    "        return example\n",
    "    \n",
    "    def get_bow_rep(self, input_pad):\n",
    "        vocab_len = self.vocab.size()\n",
    "        num_sents = input_pad.shape[0]\n",
    "        bow_rep = np.zeros((num_sents, vocab_len))\n",
    "        \n",
    "        for i, sent in enumerate(input_pad):\n",
    "            for word in sent:\n",
    "                if word not in self.filterids:\n",
    "                    bow_rep[i][word]+=1\n",
    "        row_norms = np.linalg.norm(bow_rep, axis=1, keepdims=True)\n",
    "        return bow_rep/row_norms\n",
    "\n",
    "    def get_bert_tokenizer(self):\n",
    "        inputs = self.tokenizer(self.text, padding=True, truncation=True, return_tensors='pt')\n",
    "        return inputs\n",
    "    \n",
    "    def AddTopicNode(self, G, num_topics):\n",
    "        wid2nid = {}\n",
    "        nid2wid = {}\n",
    "        nid = 0\n",
    "        for k in range(num_topics):\n",
    "            node_id = k \n",
    "            wid2nid[node_id] = nid\n",
    "            nid2wid[nid] = node_id\n",
    "            nid += 1\n",
    "\n",
    "        G.add_nodes(nid)\n",
    "        G.ndata[\"unit\"] = torch.zeros(nid)\n",
    "        G.ndata[\"dtype\"] = torch.zeros(nid)\n",
    "        G.ndata['id']= torch.LongTensor(list(nid2wid.values()))\n",
    "        return wid2nid, nid2wid \n",
    "    \n",
    "    def create_graph(self, input_pad, bow_rep, inputs, labels, num_topics):\n",
    "        G = dgl.graph(([], []))\n",
    "        _, _ = self.AddTopicNode(G, num_topics)\n",
    "        N = len(input_pad)\n",
    "        G.add_nodes(N)\n",
    "        G.ndata[\"unit\"][num_topics:] = torch.ones(N)\n",
    "        G.ndata[\"dtype\"][num_topics:] = torch.ones(N)\n",
    "        sentids = [i+num_topics for i in range(N)]\n",
    "        G.nodes[sentids].data['bert_input_ids'] = inputs['input_ids']\n",
    "        G.nodes[sentids].data['bert_attention_mask'] = inputs['attention_mask']\n",
    "        G.nodes[sentids].data['bert_token_type_ids'] = inputs['token_type_ids']\n",
    "        G.nodes[sentids].data['bow'] = bow_rep\n",
    "        G.nodes[sentids].data['label'] = labels\n",
    "        for i in range(N): \n",
    "            G.nodes[i+num_topics].data['id'] = torch.LongTensor([i])\n",
    "            \n",
    "        for i in range(num_topics):\n",
    "            for j in range(N):\n",
    "                G.add_edge(i, j+num_topics, data={'tfidfembed': torch.tensor(1.0), 'dtype': torch.tensor(0.0)})\n",
    "                G.add_edge(j+num_topics, i, data={'tfidfembed': torch.tensor(1.0), 'dtype': torch.tensor(0.0)})\n",
    "        return G\n",
    "    \n",
    "    def checker(self, index):\n",
    "        item = self.get_example(index)\n",
    "        input_pad = np.array(item.enc_sent_input_pad[:self.doc_max_timesteps])\n",
    "        bow_rep = torch.tensor(self.get_bow_rep(input_pad), dtype=torch.float32)\n",
    "        labels = torch.tensor(item.labels, dtype=torch.int32)\n",
    "        inputs = self.get_bert_tokenizer()\n",
    "        G = self.create_graph(input_pad, bow_rep, inputs, labels, self.num_topics)\n",
    "        return G\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        item = self.get_example(index)\n",
    "        input_pad = np.array(item.enc_sent_input_pad[:self.doc_max_timesteps])\n",
    "        bow_rep = torch.tensor(self.get_bow_rep(input_pad), dtype=torch.float32)\n",
    "        labels = torch.tensor(item.labels, dtype=torch.int32)\n",
    "        inputs = self.get_bert_tokenizer()\n",
    "        G = self.create_graph(input_pad, bow_rep, inputs, labels, self.num_topics)\n",
    "        return G\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=21, num_edges=0,\n",
       "      ndata_schemes={'unit': Scheme(shape=(), dtype=torch.float32), 'dtype': Scheme(shape=(), dtype=torch.float32), 'id': Scheme(shape=(), dtype=torch.int64), 'bert_input_ids': Scheme(shape=(36,), dtype=torch.int64), 'bert_attention_mask': Scheme(shape=(36,), dtype=torch.int64), 'bert_token_type_ids': Scheme(shape=(36,), dtype=torch.int64), 'bow': Scheme(shape=(50000,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int32)}\n",
       "      edata_schemes={})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = ExampleSet(data_path=DATA_FILE, vocab=vocab, doc_max_timesteps=doc_max_timesteps, sent_max_len=sent_max_len, filter_word_path=FILTER_WORD, w2s_path=train_w2s_path, num_topics=5)\n",
    "G = dataset.checker(0)\n",
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sample_graph.pkl', 'wb') as f:\n",
    "    pkl.dump(G, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    G.add_edges([0, 1, 2, 3, 4], [5, 6, 7, 8, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
