{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/hitesh.goel/miniconda3/envs/inlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import glob\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import nltk\n",
    "import collections\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import torch\n",
    "import argparse\n",
    "import time\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import networkx as nx\n",
    "import pickle as pkl\n",
    "import dgl \n",
    "from module.vocabulary import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../cnndm'\n",
    "cache_dir = '../cache/CNNDM'\n",
    "DATA_FILE = os.path.join(data_dir, \"index_to_file_mapping_train.json\")\n",
    "VALID_FILE = os.path.join(data_dir, \"index_to_file_mapping_val.json\")\n",
    "VOCAL_FILE = os.path.join(cache_dir, \"vocab\")\n",
    "FILTER_WORD = os.path.join(cache_dir, \"filter_word.txt\")\n",
    "train_w2s_path = os.path.join(cache_dir, \"index_to_file_mapping_train.json\")\n",
    "val_w2s_path = os.path.join(cache_dir, \"index_to_file_mapping_val.json\")\n",
    "    \n",
    "# defaults\n",
    "vocab_size = 50000\n",
    "doc_max_timesteps = 50\n",
    "sent_max_len = 100\n",
    "\n",
    "# vocab\n",
    "vocab = Vocab(VOCAL_FILE, vocab_size)\n",
    "\n",
    "# filterwords\n",
    "FILTERWORD = stopwords.words('english')\n",
    "punctuations = [',', '.', ':', ';', '?', '(', ')', '[', ']', '&', '!', '*', '@', '#', '$', '%', '\\'\\'', '\\'', '`', '``',\n",
    "                '-', '--', '|', '\\/']\n",
    "FILTERWORD.extend(punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def readJson(fname):\n",
    "    data = []\n",
    "    with open(fname, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "\n",
    "def readText(fname):\n",
    "    data = []\n",
    "    with open(fname, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data.append(line.strip())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Example(object):\n",
    "    def __init__(self, article_sents, abstract_sents, vocab, sent_max_len, label):\n",
    "        self.sent_max_len = sent_max_len\n",
    "        self.enc_sent_len = []\n",
    "        self.enc_sent_input = []\n",
    "        self.enc_sent_input_pad = []\n",
    "\n",
    "        # Store the original strings\n",
    "        self.original_article_sents = article_sents\n",
    "        self.original_abstract = \"\\n\".join(abstract_sents)\n",
    "\n",
    "        # Process the article\n",
    "        if isinstance(article_sents, list) and isinstance(article_sents[0], list):  # multi document\n",
    "            self.original_article_sents = []\n",
    "            for doc in article_sents:\n",
    "                self.original_article_sents.extend(doc)\n",
    "        for sent in self.original_article_sents:\n",
    "            article_words = sent.split()\n",
    "            self.enc_sent_len.append(len(article_words))  # store the length before padding\n",
    "            self.enc_sent_input.append([vocab.word2id(w.lower()) for w in article_words])  # list of word ids; OOVs are represented by the id for UNK token\n",
    "        self._pad_encoder_input(vocab.word2id('[PAD]'))\n",
    "\n",
    "        # Store the label\n",
    "        self.labels = np.array([1 if i in label else 0 for i in range(len(self.original_article_sents))])\n",
    "        \n",
    "    def _pad_encoder_input(self, pad_id):\n",
    "        \"\"\"\n",
    "        :param pad_id: int; token pad id\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        max_len = self.sent_max_len\n",
    "        for i in range(len(self.enc_sent_input)):\n",
    "            article_words = self.enc_sent_input[i].copy()\n",
    "            if len(article_words) > max_len:\n",
    "                article_words = article_words[:max_len]\n",
    "            if len(article_words) < max_len:\n",
    "                article_words.extend([pad_id] * (max_len - len(article_words)))\n",
    "            self.enc_sent_input_pad.append(article_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_path, vocab, doc_max_timesteps, sent_max_len, filter_word_path, w2s_path, num_topics):\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.sent_max_len = sent_max_len\n",
    "        self.doc_max_timesteps = doc_max_timesteps\n",
    "        self.num_topics = num_topics\n",
    "        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.example_list = json.load(f)\n",
    "        self.size = len(self.example_list)\n",
    "\n",
    "        tfidf_w = readText(filter_word_path)\n",
    "        self.filterwords = FILTERWORD\n",
    "        self.filterids = [vocab.word2id(w.lower()) for w in FILTERWORD]\n",
    "        self.filterids.append(vocab.word2id(\"[PAD]\"))   # keep \"[UNK]\" but remove \"[PAD]\"\n",
    "        lowtfidf_num = 0\n",
    "        for w in tfidf_w:\n",
    "            if vocab.word2id(w) != vocab.word2id('[UNK]'):\n",
    "                self.filterwords.append(w)\n",
    "                self.filterids.append(vocab.word2id(w))\n",
    "                lowtfidf_num += 1\n",
    "            if lowtfidf_num > 5000:\n",
    "                break\n",
    "        self.filterids = list(set(self.filterids))\n",
    "        self.filterwords = list(set(self.filterwords))\n",
    "        with open(w2s_path, \"r\") as f:\n",
    "            self.w2s_tfidf = json.load(f)  \n",
    "            \n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')      \n",
    "            \n",
    "    def get_example(self, index):\n",
    "        file_name, new_index  = self.example_list[str(index)]\n",
    "        e = readJson(file_name)\n",
    "        e = e[new_index]\n",
    "        e[\"summary\"] = e.setdefault(\"summary\", [])\n",
    "        self.text = e['text']\n",
    "        example = Example(e[\"text\"], e[\"summary\"], self.vocab, self.sent_max_len, e[\"label\"])\n",
    "        return example\n",
    "    \n",
    "    def get_bow_rep(self, input_pad):\n",
    "        vocab_len = self.vocab.size()\n",
    "        num_sents = input_pad.shape[0]\n",
    "        bow_rep = np.zeros((num_sents, vocab_len))\n",
    "        for i, sent in enumerate(input_pad):\n",
    "            for word in sent:\n",
    "                if word not in self.filterids:\n",
    "                    bow_rep[i][int(word)]+=1\n",
    "        return bow_rep\n",
    "\n",
    "    def get_bert_tokenizer(self):\n",
    "        inputs = self.tokenizer(self.text, truncation=True, return_tensors='pt', max_length=100, padding='max_length')\n",
    "        return inputs\n",
    "    \n",
    "    def AddTopicNode(self, G, num_topics):\n",
    "        wid2nid = {}\n",
    "        nid2wid = {}\n",
    "        nid = 0\n",
    "        for k in range(num_topics):\n",
    "            node_id = k \n",
    "            wid2nid[node_id] = nid\n",
    "            nid2wid[nid] = node_id\n",
    "            nid += 1\n",
    "\n",
    "        G.add_nodes(nid)\n",
    "        G.ndata[\"unit\"] = torch.zeros(nid)\n",
    "        G.ndata[\"dtype\"] = torch.zeros(nid)\n",
    "        G.ndata['id']= torch.LongTensor(list(nid2wid.values()))\n",
    "        return wid2nid, nid2wid \n",
    "    \n",
    "    def create_graph(self, input_pad, bow_rep, inputs, labels, num_topics):\n",
    "        G = dgl.graph(([], []))\n",
    "        _, _ = self.AddTopicNode(G, num_topics)\n",
    "        N = len(input_pad)\n",
    "        G.add_nodes(N)\n",
    "        G.ndata[\"unit\"][num_topics:] = torch.ones(N)\n",
    "        G.ndata[\"dtype\"][num_topics:] = torch.ones(N)\n",
    "        sentids = [i+num_topics for i in range(N)]\n",
    "        G.nodes[sentids].data['bert_input_ids'] = inputs['input_ids'][:len(sentids)]\n",
    "        G.nodes[sentids].data['bert_attention_mask'] = inputs['attention_mask'][:len(sentids)]\n",
    "        G.nodes[sentids].data['bert_token_type_ids'] = inputs['token_type_ids'][:len(sentids)]\n",
    "        G.nodes[sentids].data['bow'] = bow_rep[:len(sentids)]\n",
    "        G.nodes[sentids].data['label'] = labels[:len(sentids)]\n",
    "        for i in range(N): \n",
    "            G.nodes[i+num_topics].data['id'] = torch.LongTensor([i])\n",
    "        # for i in range(num_topics):\n",
    "        #     for j in range(N):\n",
    "        #         G.add_edges([i], [j+num_topics], data={'tfidfembed': torch.tensor(1.0), 'dtype': torch.tensor(1.0)})\n",
    "        #         G.add_edges([j+num_topics], [i], data={'tfidfembed': torch.tensor(1.0), 'dtype': torch.tensor(0.0)})\n",
    "        \n",
    "        return G\n",
    "    \n",
    "    def checker(self, index):\n",
    "        item = self.get_example(index)\n",
    "        sents = np.array(item.enc_sent_input_pad)\n",
    "        labels = item.labels\n",
    "        if sents.shape[0] < self.doc_max_timesteps:\n",
    "            num_pad_sents = self.doc_max_timesteps - len(sents)\n",
    "            pad_arr = np.zeros((num_pad_sents, len(sents[0])))\n",
    "            input_pad = np.concatenate((sents, pad_arr), axis=0) \n",
    "            pad_l = np.zeros(num_pad_sents)\n",
    "            labels = np.concatenate((labels, pad_l), axis=0)  \n",
    "            for _ in range(num_pad_sents):\n",
    "                self.text.append(\"\")  \n",
    "        else:\n",
    "            input_pad = sents[:self.doc_max_timesteps]\n",
    "            labels = labels[:self.doc_max_timesteps]\n",
    "            self.text = self.text[:self.doc_max_timesteps]\n",
    "            \n",
    "        bow_rep = torch.tensor(self.get_bow_rep(input_pad), dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int32)\n",
    "        inputs = self.get_bert_tokenizer()\n",
    "        \n",
    "        G = self.create_graph(input_pad, bow_rep, inputs, labels, self.num_topics)\n",
    "        return G, index\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        item = self.get_example(index)\n",
    "        input_pad = np.array(item.enc_sent_input_pad[:self.doc_max_timesteps])\n",
    "        bow_rep = torch.tensor(self.get_bow_rep(input_pad), dtype=torch.float32)\n",
    "        labels = torch.tensor(item.labels, dtype=torch.int32)\n",
    "        inputs = self.get_bert_tokenizer()\n",
    "        G = self.create_graph(input_pad, bow_rep, inputs, labels, self.num_topics)\n",
    "        return G\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Graph(num_nodes=55, num_edges=0,\n",
       "       ndata_schemes={'unit': Scheme(shape=(), dtype=torch.float32), 'dtype': Scheme(shape=(), dtype=torch.float32), 'id': Scheme(shape=(), dtype=torch.int64), 'bert_input_ids': Scheme(shape=(100,), dtype=torch.int64), 'bert_attention_mask': Scheme(shape=(100,), dtype=torch.int64), 'bert_token_type_ids': Scheme(shape=(100,), dtype=torch.int64), 'bow': Scheme(shape=(50000,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int32)}\n",
       "       edata_schemes={}),\n",
       " 1)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = ExampleSet(data_path=DATA_FILE, vocab=vocab, doc_max_timesteps=doc_max_timesteps, sent_max_len=sent_max_len, filter_word_path=FILTER_WORD, w2s_path=train_w2s_path, num_topics=5)\n",
    "G = dataset.checker(1)\n",
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sample_graph.pkl', 'wb') as f:\n",
    "    pkl.dump(G, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
