{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import glob\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import nltk\n",
    "import collections\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import torch\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "from module.vocabulary import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../cnndm'\n",
    "cache_dir = '../cache/CNNDM'\n",
    "DATA_FILE = os.path.join(data_dir, \"index_to_file_mapping_train.json\")\n",
    "VALID_FILE = os.path.join(data_dir, \"index_to_file_mapping_val.json\")\n",
    "VOCAL_FILE = os.path.join(cache_dir, \"vocab\")\n",
    "FILTER_WORD = os.path.join(cache_dir, \"filter_word.txt\")\n",
    "train_w2s_path = os.path.join(cache_dir, \"index_to_file_mapping_train.json\")\n",
    "val_w2s_path = os.path.join(cache_dir, \"index_to_file_mapping_val.json\")\n",
    "    \n",
    "# defaults\n",
    "vocab_size = 50000\n",
    "doc_max_timesteps = 50\n",
    "sent_max_len = 100\n",
    "\n",
    "# vocab\n",
    "vocab = Vocab(VOCAL_FILE, vocab_size)\n",
    "\n",
    "# filterwords\n",
    "FILTERWORD = stopwords.words('english')\n",
    "punctuations = [',', '.', ':', ';', '?', '(', ')', '[', ']', '&', '!', '*', '@', '#', '$', '%', '\\'\\'', '\\'', '`', '``',\n",
    "                '-', '--', '|', '\\/']\n",
    "FILTERWORD.extend(punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def readJson(fname):\n",
    "    data = []\n",
    "    with open(fname, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "\n",
    "def readText(fname):\n",
    "    data = []\n",
    "    with open(fname, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data.append(line.strip())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Example(object):\n",
    "    def __init__(self, article_sents, abstract_sents, vocab, sent_max_len, label):\n",
    "        self.sent_max_len = sent_max_len\n",
    "        self.enc_sent_len = []\n",
    "        self.enc_sent_input = []\n",
    "        self.enc_sent_input_pad = []\n",
    "\n",
    "        # Store the original strings\n",
    "        self.original_article_sents = article_sents\n",
    "        self.original_abstract = \"\\n\".join(abstract_sents)\n",
    "\n",
    "        # Process the article\n",
    "        if isinstance(article_sents, list) and isinstance(article_sents[0], list):  # multi document\n",
    "            self.original_article_sents = []\n",
    "            for doc in article_sents:\n",
    "                self.original_article_sents.extend(doc)\n",
    "        for sent in self.original_article_sents:\n",
    "            article_words = sent.split()\n",
    "            self.enc_sent_len.append(len(article_words))  # store the length before padding\n",
    "            self.enc_sent_input.append([vocab.word2id(w.lower()) for w in article_words])  # list of word ids; OOVs are represented by the id for UNK token\n",
    "        self._pad_encoder_input(vocab.word2id('[PAD]'))\n",
    "\n",
    "        # Store the label\n",
    "        self.label = label\n",
    "        label_shape = (len(self.original_article_sents), len(label))  # [N, len(label)]\n",
    "        # label_shape = (len(self.original_article_sents), len(self.original_article_sents))\n",
    "        self.label_matrix = np.zeros(label_shape, dtype=int)\n",
    "        if label != []:\n",
    "            self.label_matrix[np.array(label), np.arange(len(label))] = 1  # label_matrix[i][j]=1 indicate the i-th sent will be selected in j-th step\n",
    "\n",
    "    def _pad_encoder_input(self, pad_id):\n",
    "        \"\"\"\n",
    "        :param pad_id: int; token pad id\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        max_len = self.sent_max_len\n",
    "        for i in range(len(self.enc_sent_input)):\n",
    "            article_words = self.enc_sent_input[i].copy()\n",
    "            if len(article_words) > max_len:\n",
    "                article_words = article_words[:max_len]\n",
    "            if len(article_words) < max_len:\n",
    "                article_words.extend([pad_id] * (max_len - len(article_words)))\n",
    "            self.enc_sent_input_pad.append(article_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_path, vocab, doc_max_timesteps, sent_max_len, filter_word_path, w2s_path):\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.sent_max_len = sent_max_len\n",
    "        self.doc_max_timesteps = doc_max_timesteps\n",
    "\n",
    "        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.example_list = json.load(f)\n",
    "        self.size = len(self.example_list)\n",
    "\n",
    "        tfidf_w = readText(filter_word_path)\n",
    "        self.filterwords = FILTERWORD\n",
    "        self.filterids = [vocab.word2id(w.lower()) for w in FILTERWORD]\n",
    "        self.filterids.append(vocab.word2id(\"[PAD]\"))   # keep \"[UNK]\" but remove \"[PAD]\"\n",
    "        lowtfidf_num = 0\n",
    "        for w in tfidf_w:\n",
    "            if vocab.word2id(w) != vocab.word2id('[UNK]'):\n",
    "                self.filterwords.append(w)\n",
    "                self.filterids.append(vocab.word2id(w))\n",
    "                lowtfidf_num += 1\n",
    "            if lowtfidf_num > 5000:\n",
    "                break\n",
    "\n",
    "        with open(w2s_path, \"r\") as f:\n",
    "            self.w2s_tfidf = json.load(f)\n",
    "            \n",
    "    def get_w2s(self,index):\n",
    "        index = str(index)\n",
    "        file_name, new_index = self.w2s_tfidf[index]\n",
    "        ws = readJson(file_name)\n",
    "        return ws[new_index]\n",
    "    \n",
    "    def pad_label_m(self, label_matrix):\n",
    "        label_m = label_matrix[:self.doc_max_timesteps, :self.doc_max_timesteps]\n",
    "        N, m = label_m.shape\n",
    "        if m < self.doc_max_timesteps:\n",
    "            pad_m = np.zeros((N, self.doc_max_timesteps - m))\n",
    "            return np.hstack([label_m, pad_m])\n",
    "        return label_m\n",
    "            \n",
    "    def get_example(self, index):\n",
    "        file_name, new_index  = self.example_list[str(index)]\n",
    "        e = readJson(file_name)\n",
    "        e = e[new_index]\n",
    "        e[\"summary\"] = e.setdefault(\"summary\", [])\n",
    "        example = Example(e[\"text\"], e[\"summary\"], self.vocab, self.sent_max_len, e[\"label\"])\n",
    "        return example\n",
    "    \n",
    "    # def checker(self, index):\n",
    "    #     item = self.get_example(index)\n",
    "    #     input_pad = item.enc_sent_input_pad[:self.doc_max_timesteps]\n",
    "    #     label = self.pad_label_m(item.label_matrix)\n",
    "    #     # w2s_w = self.w2s_tfidf[index]\n",
    "    #     w2s_w = self.get_w2s(index)\n",
    "    #     return input_pad, label, w2s_w, index\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item = self.get_example(index)\n",
    "        input_pad = item.enc_sent_input_pad[:self.doc_max_timesteps]\n",
    "        label = self.pad_label_m(item.label_matrix)\n",
    "        # w2s_w = self.w2s_tfidf[index]\n",
    "        w2s_w = self.get_w2s(index)\n",
    "        return input_pad, label, w2s_w, index\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ExampleSet(data_path=DATA_FILE, vocab=vocab, doc_max_timesteps=doc_max_timesteps, sent_max_len=sent_max_len, filter_word_path=FILTER_WORD, w2s_path=train_w2s_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/scratch/hitesh.goel/cnndm/train/0.jsonl', 0]\n",
      "[[21, 13, 8, 694, 10, 11130, 15, 592, 440, 45, 238, 12, 144, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [16235, 7, 5, 2080, 6, 39, 73, 34, 210, 19404, 9, 16959, 17, 8, 1609, 990, 27, 12861, 3108, 12157, 60, 29, 41814, 4575, 17818, 5, 16790, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [33, 72, 309, 662, 8, 287, 61, 1264, 9, 1037, 8, 26514, 6213, 27, 5, 6820, 4579, 11, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [37, 21, 13, 42, 271, 5, 77, 694, 15, 5, 238, 11, 5, 426, 15880, 2836, 33, 2080, 9, 120, 8, 8434, 14, 5, 4397, 12069, 57, 64, 589, 33, 7, 663, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3159, 22, 185, 24, 5, 238, 1, 130, 1, 73, 34, 210, 3392, 110, 27, 33, 309, 277, 14, 5, 4397, 12069, 128, 5, 990, 7, 663, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [39, 13, 1226, 7, 37786, 5, 342, 9, 13, 6690, 124, 5, 1241, 26, 1, 990, 6, 11, 20771, 6, 96, 100, 4579, 213, 29, 5, 3108, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [5794, 1813, 73, 34, 210, 1425, 33, 100, 938, 466, 29, 5, 487, 10, 5, 342, 6, 1226, 7, 1171, 33, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [56, 120, 8, 20970, 1221, 7, 2351, 33, 7, 663, 29, 2038, 25, 9643, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [21, 19, 1120, 15, 39, 2182, 33, 3581, 9, 1237, 31, 5, 342, 22, 80, 7291, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [5, 1230, 13, 364, 17, 430, 780, 9, 5330, 7, 161, 3131, 496, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [10187, 33, 2380, 27, 5, 3108, 2009, 104, 17, 33, 6, 5, 238, 73, 34, 210, 745, 308, 5, 990, 7, 605, 9, 145, 21, 96, 5, 6820, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [37, 21, 19, 241, 549, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [5, 342, 20306, 33, 128, 33, 949, 9, 39, 19, 938, 466, 29, 5, 3108, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [95, 26130, 62, 10, 529, 6, 39, 19, 6690, 124, 5, 990, 1241, 27, 33, 309, 509, 6, 1226, 7, 188, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [27, 39, 19, 4579, 213, 6, 56, 1, 120, 8, 2455, 1254, 7, 1136, 33, 22, 41, 9643, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [6665, 6, 39, 199, 1237, 6690, 9, 10155, 37, 1664, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]]\n",
      "{'0': {'this': 0.3450604786586309, 'was': 0.2443696963411905, 'life': 0.3450604786586309, 'woman': 0.2443696963411905, 'nearly': 0.3450604786586309, 'it': 0.2243344612915379, 'madness': 0.3450604786586309, 'claimed': 0.3450604786586309, 'of': 0.2688908422715474, 'moment': 0.30050409767862135, 'that': 0.2688908422715474}, '1': {'by': 0.15292165511753253, 'spanish': 0.23521673490704248, 'skipping': 0.23521673490704248, 'up': 0.23521673490704248, 'as': 0.14137450365487142, 'whipped': 0.23521673490704248, 'and': 0.11465662213004135, 'hopping': 0.23521673490704248, 'lash': 0.23521673490704248, 'be': 0.1665790365417803, 'force': 0.23521673490704248, 'she': 0.1313719060835646, 'to': 0.10751710718682285, 'waves': 0.1665790365417803, 'the': 0.2019984814115872, 'beach': 0.15292165511753253, 'ferocious': 0.23521673490704248, 'oblivious': 0.23521673490704248, 'seen': 0.1665790365417803, 'storms': 0.23521673490704248, 'on': 0.18329432049530353, 'danger': 0.20484406952927145, 'gale': 0.23521673490704248, 'can': 0.1665790365417803, 'shoreline': 0.23521673490704248}, '2': {'tide': 0.25600299522501296, 'friends': 0.2290712890168682, 'beat': 0.29396110320218993, 'the': 0.12622336685436955, 'hasty': 0.29396110320218993, 'two': 0.29396110320218993, 'her': 0.13436904246731068, 'as': 0.17668217814294537, 'retreat': 0.29396110320218993, 'and': 0.1432916205732709, 'in': 0.2290712890168682, 'sense': 0.29396110320218993, 'little': 0.29396110320218993, 'swept': 0.2290712890168682, 'more': 0.29396110320218993, 'showed': 0.29396110320218993}, '3': {'woman': 0.14934568406915297, 'sprint': 0.21088250303569342, 'ramp': 0.18365202684843923, 'black': 0.21088250303569342, 'made': 0.1643316965247055, 'and': 0.10279487755816505, 'last': 0.21088250303569342, 'lead': 0.21088250303569342, 'but': 0.1643316965247055, 'that': 0.1643316965247055, 'to': 0.0963939776295028, 'until': 0.21088250303569342, 'the': 0.36220165530585385, 'which': 0.21088250303569342, 'for': 0.18365202684843923, 'her': 0.1927879552590056, 'would': 0.21088250303569342, 'was': 0.14934568406915297, 'safety': 0.1643316965247055, 'concrete': 0.18365202684843923, 'realised': 0.21088250303569342, 'danger': 0.18365202684843923, 'in': 0.1643316965247055, 'it': 0.13710122033745137, 'swimsuit': 0.21088250303569342, 'not': 0.21088250303569342, 'moment': 0.18365202684843923}, '4': {'friends': 0.19652458211006177, 'woman': 0.17860278188766293, 'beach': 0.16395960489306108, 'hanging': 0.2521947783651274, 'as': 0.15157897515162175, 'head': 0.2521947783651274, 'right': 0.2521947783651274, 'be': 0.17860278188766293, 'back': 0.2521947783651274, 'to': 0.11527773747967536, 'the': 0.3248682259139864, 'ramp': 0.21962980114812666, 'for': 0.21962980114812666, 'her': 0.11527773747967536, 'with': 0.19652458211006177, 'safety': 0.19652458211006177, 'concrete': 0.21962980114812666, 'seen': 0.17860278188766293, 'off': 0.21962980114812666, 'dancing': 0.2521947783651274, 'death': 0.2521947783651274, 'can': 0.17860278188766293}, '5': {'santander': 0.24623152730683476, 'beach': 0.16008271143105515, 'being': 0.21443656260203361, 'wall': 0.21443656260203361, 'waves': 0.17437964437859194, 'at': 0.24623152730683476, 'and': 0.12002579320761345, 'battered': 0.19187767613585627, 'unable': 0.19187767613585627, 'swept': 0.19187767613585627, 'she': 0.13752382496487778, 'to': 0.11255194714222438, 'before': 0.21443656260203361, 'sardinero': 0.24623152730683476, 'against': 0.21443656260203361, 'the': 0.31718658078023004, 'by': 0.16008271143105515, 'was': 0.34875928875718387, 'outrun': 0.24623152730683476, 'in': 0.19187767613585627, 'away': 0.21443656260203361, 'water': 0.17437964437859194}, '6': {'the': 0.24097114313570747, 'by': 0.18242565877904876, 'power': 0.28059837430019086, 'carried': 0.24436574599024372, 'save': 0.28059837430019086, 'her': 0.25652193070365237, 'being': 0.24436574599024372, 'of': 0.21865828708899587, 'witnesses': 0.28059837430019086, 'watching': 0.28059837430019086, 'horrified': 0.28059837430019086, 'seen': 0.19871803281593878, 'unable': 0.21865828708899587, 'along': 0.24436574599024372, 'be': 0.19871803281593878, 'water': 0.19871803281593878, 'can': 0.19871803281593878, 'to': 0.12826096535182618}, '7': {'attempt': 0.3297137369678614, 'his': 0.3297137369678614, 'by': 0.2143570711158721, 'pull': 0.3297137369678614, 'her': 0.15071150108665202, 'futile': 0.3297137369678614, 'one': 0.28713902387469264, 'safety': 0.25693178420904084, 'offering': 0.3297137369678614, 'umbrella': 0.28713902387469264, 'made': 0.25693178420904084, 'to': 0.30142300217330403}, '8': {'the': 0.13403564032442208, 'is': 0.20294164411449034, 'with': 0.24324907240623012, 'survived': 0.3121550761962984, 'water': 0.22106629387979207, 'understand': 0.3121550761962984, 'and': 0.15216029008972382, 'from': 0.3121550761962984, 'that': 0.24324907240623012, 'it': 0.20294164411449034, 'her': 0.14268547176106308, 'bruises': 0.3121550761962984, 'emerged': 0.27184764790455856, 'ordeal': 0.3121550761962984, 'she': 0.1743430686161619, 'just': 0.3121550761962984}, '9': {'uploaded': 0.30514334272636096, 'site': 0.30514334272636096, 'the': 0.1310248861926, 'shot': 0.30514334272636096, 'on': 0.23778512902493001, 'january': 0.30514334272636096, 'sharing': 0.30514334272636096, 'was': 0.21610062761301255, 'liveleak': 0.30514334272636096, 'video': 0.30514334272636096, 'and': 0.14874241391158158, 'com': 0.30514334272636096, 'footage': 0.30514334272636096, 'to': 0.1394804221741337}, '10': {'woman': 0.16659625400180006, 'beach': 0.15293746096284058, 'down': 0.23524104669808962, 'across': 0.23524104669808962, 'as': 0.14138911599694653, 'and': 0.11466847293581203, 'try': 0.23524104669808962, 'her': 0.21505644011744707, 'be': 0.16659625400180006, 'realising': 0.23524104669808962, 'to': 0.10752822005872353, 'tide': 0.20486524202882855, 'before': 0.20486524202882855, 'waves': 0.16659625400180006, 'the': 0.4040387195874103, 'on': 0.18331326563210162, 'seen': 0.16659625400180006, 'make': 0.23524104669808962, 'bear': 0.23524104669808962, 'mistake': 0.23524104669808962, 'it': 0.15293746096284058, 'running': 0.23524104669808962, 'can': 0.16659625400180006}, '11': {'too': 0.5381805565039258, 'is': 0.34988778109338087, 'but': 0.4193810421148649, 'it': 0.34988778109338087, 'late': 0.5381805565039258}, '12': {'waves': 0.24661409217293262, 'sweeps': 0.3482296617103587, 'by': 0.22639484495357698, 'feet': 0.3482296617103587, 'her': 0.3183501877835369, 'carried': 0.3032640558663569, 'and': 0.16974488126015264, 'is': 0.22639484495357698, 'off': 0.3032640558663569, 'along': 0.3032640558663569, 'the': 0.299051268081594, 'she': 0.19449123988479874, 'water': 0.24661409217293262}, '13': {'wall': 0.25070497843953937, 'friends': 0.22433062755228364, 'against': 0.25070497843953937, 'help': 0.28787753821236367, 'beach': 0.18715806777945934, 'is': 0.18715806777945934, 'of': 0.22433062755228364, 'control': 0.28787753821236367, 'now': 0.28787753821236367, 'as': 0.1730257164493132, 'her': 0.1315882568682433, 'watch': 0.28787753821236367, 'battered': 0.22433062755228364, 'unable': 0.22433062755228364, 'the': 0.12361115711937928, 'flailing': 0.28787753821236367, 'out': 0.28787753821236367, 'she': 0.1607837168922036, 'to': 0.1315882568682433}, '14': {'one': 0.26735155333942984, 'bypasser': 0.3069923361381013, 'desperate': 0.3069923361381013, 'reach': 0.3069923361381013, 'is': 0.1995851875470905, 'with': 0.23922597034576193, 'made': 0.23922597034576193, 'as': 0.18451446137336072, 'effort': 0.3069923361381013, 'an': 0.3069923361381013, 'her': 0.14032559342828096, 'swept': 0.23922597034576193, 'to': 0.14032559342828096, 'umbrella': 0.26735155333942984, 'she': 0.1714596045534226, 'away': 0.26735155333942984}, '15': {'and': 0.19086465521217486, 'alive': 0.3915566338353108, 'bruised': 0.3915566338353108, 'emerged': 0.34099637662986043, 'battered': 0.30512330324895964, 'she': 0.2186899726626085, 'but': 0.30512330324895964, 'later': 0.3915566338353108, 'luckily': 0.3915566338353108}}\n"
     ]
    }
   ],
   "source": [
    "dataset.checker(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "notes\n",
    "- dont make 2d matrix for labels\n",
    "- only 1D with [:max_timesteps] length?\n",
    "    - reduces space?\n",
    "- remove tf-idf scores\n",
    "- use nx to make the graphs\n",
    "- then use torch_geometric to write the code\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "# Generate sample bipartite graph data\n",
    "def generate_bipartite_data(num_nodes_src, num_nodes_dst, num_edges):\n",
    "    G = nx.complete_bipartite_graph(num_nodes_src, num_nodes_dst)\n",
    "    edge_index = np.array(G.edges()).T\n",
    "    src_features = torch.randn(num_nodes_src, 16)\n",
    "    dst_features = torch.randn(num_nodes_dst, 16)\n",
    "    return Data(edge_index=torch.tensor(edge_index, dtype=torch.long), \n",
    "                x_src=src_features, x_dst=dst_features)\n",
    "\n",
    "\n",
    "# Define the neural network model\n",
    "class GATModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GATModel, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels=16, out_channels=8, heads=2, dropout=0.6)\n",
    "        self.fc = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        src, dst = data.x_src, data.x_dst\n",
    "        x_src = self.conv1(src, data.edge_index[:2])  # Pass source node features and edge indices\n",
    "        x_dst = self.conv1(dst, data.edge_index[::-1][:2])  # Pass destination node features and edge indices\n",
    "        x = torch.cat([x_src, x_dst], dim=0)  # Concatenate source and destination node features\n",
    "        x = F.relu(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Generate sample data\n",
    "data = generate_bipartite_data(10, 10, 50)\n",
    "\n",
    "# Create model, optimizer and loss function\n",
    "model = GATModel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = criterion(output, torch.randn_like(output))  # Random target for demonstration\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
